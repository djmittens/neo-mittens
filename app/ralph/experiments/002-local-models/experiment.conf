# 002-local-models: How far can Devstral go alone on a complex C spec?
#
# Zero cloud dependency. Zero quota burn. Pure local GPU inference.
# Backend: llama.cpp llama-server with --jinja (native tool calling)
# Model: Devstral Small 2 24B Q4_K_M via llama.cpp on port 8080
#
# Target: Valkyria system-architecture-refactor (same as 001)
# Machine: redbox (Ryzen 9 9900X, 64GB RAM, RTX 5090 32GB VRAM)
#
# Single profile: all-Devstral (reasoning + build)
# This validates the local inference pipeline end-to-end before
# burning cloud quota on experiment 001.
#
# Start server before running:
#   llama-server \
#     -m ~/.lmstudio/models/lmstudio-community/Devstral-Small-2-24B-Instruct-2512-GGUF/Devstral-Small-2-24B-Instruct-2512-Q4_K_M.gguf \
#     --jinja --ctx-size 131072 --flash-attn on --port 8080 -ngl 99 \
#     --cache-type-k q8_0 --cache-type-v q8_0

# ─────────────────────────────────────────────────────────────────────
# TARGET PROJECT
# ─────────────────────────────────────────────────────────────────────

TARGET_REPO="$HOME/src/valkyria"
BASE_REF="ralph-experiment-refactor-spec"
SPEC="system-refactor-00-heap-rename.md"

# ─────────────────────────────────────────────────────────────────────
# PROFILES TO TEST
# ─────────────────────────────────────────────────────────────────────

PROFILES="devstral"

# ─────────────────────────────────────────────────────────────────────
# EXPERIMENT PARAMETERS
# ─────────────────────────────────────────────────────────────────────

# Generous iterations -- local inference is free
MAX_ITERATIONS=40
MAX_WALL_TIME=10800
MAX_FAILURES=5

# ─────────────────────────────────────────────────────────────────────
# CORRECTNESS CHECKS (valkyria-specific)
# ─────────────────────────────────────────────────────────────────────

BUILD_CMD="make build"
TEST_CMD="make test"
TEST_TIMEOUT=300

GATE_CHECKS="Phase0-OldSymbols|grep -rcE 'heap2_|tlab2_|page2_|stats2_|mark_ctx2_|valk_gc_malloc_' src/ test/ && exit 1 || exit 0"
